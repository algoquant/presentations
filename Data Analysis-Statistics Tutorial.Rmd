# Statistics Tutorial
===
***

### View Results of the entire notebook by clicking the "play" button in the navigation bar, or view results of individual cells using the "play" button associated with each cell.

### Log in as an RCloud Data Scientist to view source code using the 'edit.html' link:
<a target="_blank" href=/edit.html?notebook=`r rcloud.session.notebook.id()`>/edit.html?notebook=`r rcloud.session.notebook.id()`</a>.

### Results may be shared with non-RCloud Data Scientists (users without a login) using the 'view.html' link:
<a target="_blank" href=/view.html?notebook=`r rcloud.session.notebook.id()`>/view.html?notebook=`r rcloud.session.notebook.id()`</a>. 
## Overview

In this session we'll cover the basics of statistical analysis in R. The objectives are:

- Master application of basic stats functions
- Learn about tools for exploratory data analysis 
- Learn to conduct hypothesis testing, ANOVA, linear regression, and generalized linear regression in R 
- Learn about sampling distributions available in R



# Basic statistical summaries

## Basic stats: mean, median, min, and max

## For now: we can use them on vectors

```{r statistics-basics-1}
mean(cars$speed)
median(cars$speed)
min(cars$speed)
max(cars$speed)
```

# Basic arithmetic (review from yesterday)

## *Vector* arithmetic: can do arithmetic with an entire vector

```{r vec-review}
cars$accel <- -(cars$speed^2) / (2 * cars$dist)
cars$timetostop <- cars$dist / (cars$speed+0)/2
cars$speed.cent <- cars$speed - mean(cars$speed)
head(cars)
```

## Dealing with missing values

These functions are capable of handling missing values.

* See the help documentation for details.
* Specifically, see `na.rm` argument in documentation



# Exploratory data analysis

## Getting a quick statistical summary

You can view by-quartile summaries of vectors or elements of a data frame with summary.

```{r statistics-summary-1}
summary(mtcars$mpg)
summary(mtcars)
```


## Correlation

You can analyze the correlation between two variables, or across a matrix of different variables, using cor.

```{r statistics-correlation}
cor(mtcars[,1:5], method='pearson')
cor(mtcars[,1:5], method='spearman')
```




## Lab: Explore the `performance` data set

Your turn:

* Generate pairwise plots and correlation matrices for appropriate (i.e. numeric) variables in `performance`

## Reading in the data (From yesterday):

```{r reformat-data}
performanceCsv <- file.path(dataRead,"performance.csv")
performance <- read.csv(performanceCsv)
performance$Click.Date <- as.Date(performance$Click.Date, format='%m/%d/%y')
numeric.cols <- c('Engine.CTR', 'LP.CTR')
removePercent <- function(x) as.numeric(sub('%', '', x))/100
performance[numeric.cols] <- lapply(performance[numeric.cols], removePercent)
dollar.cols <- c('Cost', 'Rev', 'Margin')
removeDollar <- function(x) as.numeric(gsub('[\\$ )]', '', sub("\\(", "-", x)))
performance[dollar.cols] <- lapply(performance[dollar.cols], removeDollar) 
```

```{r}
```

* Do you see evidence of any outliers? How should you handle them? 

Hint: Experiment with `quantile()`, `hist()`, `summary()`, and `sd()`. You can get help from the left hand panel.

```{r}
```


# Regression

## Using `lm()` for regression

Let's use `lm()` to model `mpg` in the `mtcars` dataset.

```{r statistics-lm}
fit0 <- lm(mpg ~ ., data=mtcars) # fit with all variables
fit0
```

## How to extract inference results?

```{r inference-lm}
summary(fit0)
```


# Model specification

## Uses a `formula`

A concept that is relevant in many different aspects of R

* Two sides separated by a "~" symbol
* Left side: dependent variables (in this example, it is `mpg`)
* Right side: independent variables, separated by a "+" in simple cases (in this example a `.` means all other variables in the dataset).

## Examples:
* mpg ~ hp
* mpg ~ hp + disp
* hp ~ cyl + am + wt
## Your turn

* Create a formula that specifies a model that predicts `mpg` by `vs`, `gear`, and `wt`

```{r}
```

* Use `lm()` as above to estimate this model, and extract some inference results.

```{r}
```

## Using factor variables in regression

Several of the numeric variables in `mtcars`, e.g. `cyl`, `gear`, and `carb` are better expressed as factors.

```{r statistics-lm-2}
new.data <- mtcars
factor.cols <- c('cyl','gear','carb')
new.data[, factor.cols] <- lapply(new.data[, factor.cols], as.factor) 
head(new.data)
str(new.data)
```



## Now fit the model with factor columns

## The new "factor" variables have the same names, so you can use the same call as before:

```{r statistics-lm-3}
fit1 <- lm(mpg ~ ., data=new.data) 
summary(fit1)
contrasts(new.data$cyl)
contr.helmert(3)
```


## Model simplification with stepwise regression

With the `MASS` package, we can use the function `stepAIC()` To do model selection.

```{r statistics-lm-aic-1}
library(MASS) # package associated with Modern Applied Statistics with S 
fit.step <-stepAIC(fit1, direction='both')
summary(fit.step)
```


## Model simplification with stepwise regression

Now explicitly fit the best model

```{r statistics-lm-aic-2}
fit2 <- lm(mpg ~ cyl + hp + wt + am, data=new.data)
summary(fit2)
```


## Getting regression summaries

```{r statistics-lm-summaries}
names(fit1)
class(fit1)
typeof(fit1)
coef(fit1)
resid(fit1)
fitted(fit1)

lm1.sum <- summary(fit1)
names(lm1.sum)
summary(fit1)$adj.r.squared
summary(fit2)$adj.r.squared

summary(fit1)$coefficients
summary(fit2)$coefficients
coef(fit1)
coef(lm1.sum)
``` 


## Anova

The function `anova()` allows us to test whether the full model is explains significantly more variance than the pruned one.

```{r statistics-anova}
anova(fit2, fit1)
```

## Plotting regression results

and `plot()` has a method for residual analysis of linear models: 

```{r statistics-anova-plot, fig.show='hold'}
par(mfrow=c(2, 3))
plot(fit2, 1:6)
par(mfrow=c(1, 1))
```


## Adding regression lines

As an aside, regression lines can easily be added to your R plots. Recall last module's `cars` example:  

```{r statistics-regression-lines}
plot(dist ~ speed, data=cars,
     xlab = "Initial Speed (mph)",
     ylab = "Stopping Distance (ft)",
     main = "Car Speed vs. Stopping Distance",
     col = "red")

regression.line <- lm(dist~speed, data=cars)
abline(regression.line) 
```



## Lab: What variables best explain ROI?

Your turn, using the `performance` data.

* What factors in the `performance` data set are the strongest predictors of `ROI`? 
    - Hint:  Explore the data, transform the data, then model it using `lm()`.

    
```{r}
```

* Can you simplify your original model?
    - Hint: Experiment with `stepAIC()` to refine your model.

```{r}
```

Bonus points: Can you efficiently apply this analysis to the three subsets of performance?  

```{r}
```


# Hypothesis testing

## Testing for normality

There are all kinds of hypotheses that statisticians may be interested in testing. 

One of the most important tests is of a sample's normality:

```{r statistics-shapiro}
cars$acceleration <- (cars$speed^2) / (2*cars$dist)
shapiro.test(cars$acceleration)
```

Helpful Link for Interpretation: [http://www.dummies.com/how-to/content/how-to-test-data-normality-in-a-formal-way-in-r.html](http://www.dummies.com/how-to/content/how-to-test-data-normality-in-a-formal-way-in-r.html)


## Testing for differences in means

The function `t.test()` allows us to test hypotheses about approximately-normally distributed data.

```{r statistics-t-test}
x.data <- rnorm(100,3,1)
y.data <- rnorm(100,3,1)
t.test(x.data)
t.test(x.data,y.data)
```

## Use `help(t.test)` to find additional arguments (esp. see `paired`)

Helpful Link for Interpretation: [http://www.stat.columbia.edu/~martin/W2024/R2.pdf](http://www.stat.columbia.edu/~martin/W2024/R2.pdf)




## Lab: Two-sample t-test

```{r statistics-load-iswr}
library(ISwR) 
```

The package `ISwR` gives you access to the dataset intake, pre- and post-menstrual caloric intake of a group of women. 

* Calculate the group intake means, then use `t.test()` to calculate whether the difference between groups is statistically significant.

```{r}
```

* Is the post-pre difference normally distributed?

```{r}
```


# Probability distributions

## Probability distributions

R supports all of the well known distribution functions

* Uniform
* Beta
* Binomial
* Chi-squared
* Exponential
* Poisson
* F-distribution
* Gamma
* Geometric
* Hypergeometric
* Log-normal
* Logistic


## An example from the normal distribution function

The function `dnorm()` provides the probability density function (PDF) 

```{r statistics-dnorm}
dnorm(5, mean = 0, sd = 1) # height of PDF at x = 5
dnorm(0, mean = 0, sd = 1) # height of PDF at x = 0
```

The function `pnorm()` provides the cumulative distribution function (CDF)

```{rstatistics-pnorm}
pnorm(5, mean = 0, sd = 1) # P(X<5 | X~N(0,1) ) 
qnorm(0.975)
```



## Plotting distributions

The function `plot()` also has a method for plotting distributions, e.g.: 

```{r statistics-distributions}
par(mfrow=c(1,2)) 
plot(dnorm, -5, 5)
plot(pnorm, -5, 5)
par(mfrow=c(1, 1)) 
```


## Sampling from a distribution

It is easy to randomly sample from a probability distribution, e.g.:

```{r statistics-sampling}
rnorm(10, mean = 0, sd = 1)
```

To make the results reproducible we can use the seed value:

```{r statistics-set-seed}
set.seed(100)
rnorm(10, mean = 0, sd = 1)
rnorm(10, mean = 0, sd = 1)
set.seed(100)
rnorm(10, mean = 0, sd = 1)
```



# Other topics 

## Time series data

R has a native time series data type

```{r statistics-time-series}
sunspots.ts <- ts(sunspots)
plot(sunspots.ts)
```


## Time series data

Time Series objects allow you to perform interesting analysis like spectral analysis or ARMA and ARIMA models.

We won't cover time series in this course but there are some good resources for time series

[Time Series Analysis and Its Applications: With R Examples by Robert H. Shumway, David S. Stoffer](http://www.amazon.com/Time-Series-Analysis-Its-Applications/dp/144197864X)


## There is much, much more in R

There's pretty much nothing out there that can be done with statistics that R doesn't handle. 

Other topics:

* logistic regression: `?glm`
* mixed-effects: `library(help=nlme); library(help=lme4)`
* neural-networks: `library(help=nnet)`
* cluster analysis: `library(help=cluster)`
* survival analysis: `library(help=survival)`
* Many others!

See other Revolution course material for in-depth tutorials on these topics and more.




## Module review questions

- What are some of the basic functions available to users for exploratory data analysis?
- What are some ways to visually and empirically test a sample's distribution?
- What are some examples of useful plot methods beyond simple scatterplots?
- How do you exactly reproduce the results of a random experiment in R? 


